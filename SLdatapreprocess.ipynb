{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from numpy import random\n",
    "from CFN_impl import CFNCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = pd.read_csv('./data/retailrocket/events.csv')\n",
    "category_tree_df = pd.read_csv('./data/retailrocket/category_tree.csv')\n",
    "item_properties_1_df = pd.read_csv('./data/retailrocket/item_properties_part1.csv')\n",
    "item_properties_2_df = pd.read_csv('./data/retailrocket/item_properties_part2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_properties_df = pd.concat([item_properties_1_df, item_properties_2_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>itemid</th>\n",
       "      <th>property</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1435460400000</td>\n",
       "      <td>460429</td>\n",
       "      <td>categoryid</td>\n",
       "      <td>1338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1441508400000</td>\n",
       "      <td>206783</td>\n",
       "      <td>888</td>\n",
       "      <td>1116713 960601 n277.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1439089200000</td>\n",
       "      <td>395014</td>\n",
       "      <td>400</td>\n",
       "      <td>n552.000 639502 n720.000 424566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1431226800000</td>\n",
       "      <td>59481</td>\n",
       "      <td>790</td>\n",
       "      <td>n15360.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1431831600000</td>\n",
       "      <td>156781</td>\n",
       "      <td>917</td>\n",
       "      <td>828513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9275898</th>\n",
       "      <td>1433646000000</td>\n",
       "      <td>236931</td>\n",
       "      <td>929</td>\n",
       "      <td>n12.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9275899</th>\n",
       "      <td>1440903600000</td>\n",
       "      <td>455746</td>\n",
       "      <td>6</td>\n",
       "      <td>150169 639134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9275900</th>\n",
       "      <td>1439694000000</td>\n",
       "      <td>347565</td>\n",
       "      <td>686</td>\n",
       "      <td>610834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9275901</th>\n",
       "      <td>1433646000000</td>\n",
       "      <td>287231</td>\n",
       "      <td>867</td>\n",
       "      <td>769062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9275902</th>\n",
       "      <td>1442113200000</td>\n",
       "      <td>275768</td>\n",
       "      <td>888</td>\n",
       "      <td>888666 n10800.000 746840 1318567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20275902 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp  itemid    property                             value\n",
       "0        1435460400000  460429  categoryid                              1338\n",
       "1        1441508400000  206783         888           1116713 960601 n277.200\n",
       "2        1439089200000  395014         400   n552.000 639502 n720.000 424566\n",
       "3        1431226800000   59481         790                        n15360.000\n",
       "4        1431831600000  156781         917                            828513\n",
       "...                ...     ...         ...                               ...\n",
       "9275898  1433646000000  236931         929                           n12.000\n",
       "9275899  1440903600000  455746           6                     150169 639134\n",
       "9275900  1439694000000  347565         686                            610834\n",
       "9275901  1433646000000  287231         867                            769062\n",
       "9275902  1442113200000  275768         888  888666 n10800.000 746840 1318567\n",
       "\n",
       "[20275902 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_properties_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>visitorid</th>\n",
       "      <th>event</th>\n",
       "      <th>itemid</th>\n",
       "      <th>transactionid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1433221332117</td>\n",
       "      <td>257597</td>\n",
       "      <td>view</td>\n",
       "      <td>355908</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1433224214164</td>\n",
       "      <td>992329</td>\n",
       "      <td>view</td>\n",
       "      <td>248676</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1433221999827</td>\n",
       "      <td>111016</td>\n",
       "      <td>view</td>\n",
       "      <td>318965</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1433221955914</td>\n",
       "      <td>483717</td>\n",
       "      <td>view</td>\n",
       "      <td>253185</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1433221337106</td>\n",
       "      <td>951259</td>\n",
       "      <td>view</td>\n",
       "      <td>367447</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756096</th>\n",
       "      <td>1438398785939</td>\n",
       "      <td>591435</td>\n",
       "      <td>view</td>\n",
       "      <td>261427</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756097</th>\n",
       "      <td>1438399813142</td>\n",
       "      <td>762376</td>\n",
       "      <td>view</td>\n",
       "      <td>115946</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756098</th>\n",
       "      <td>1438397820527</td>\n",
       "      <td>1251746</td>\n",
       "      <td>view</td>\n",
       "      <td>78144</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756099</th>\n",
       "      <td>1438398530703</td>\n",
       "      <td>1184451</td>\n",
       "      <td>view</td>\n",
       "      <td>283392</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756100</th>\n",
       "      <td>1438400163914</td>\n",
       "      <td>199536</td>\n",
       "      <td>view</td>\n",
       "      <td>152913</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2756101 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp  visitorid event  itemid  transactionid\n",
       "0        1433221332117     257597  view  355908            NaN\n",
       "1        1433224214164     992329  view  248676            NaN\n",
       "2        1433221999827     111016  view  318965            NaN\n",
       "3        1433221955914     483717  view  253185            NaN\n",
       "4        1433221337106     951259  view  367447            NaN\n",
       "...                ...        ...   ...     ...            ...\n",
       "2756096  1438398785939     591435  view  261427            NaN\n",
       "2756097  1438399813142     762376  view  115946            NaN\n",
       "2756098  1438397820527    1251746  view   78144            NaN\n",
       "2756099  1438398530703    1184451  view  283392            NaN\n",
       "2756100  1438400163914     199536  view  152913            NaN\n",
       "\n",
       "[2756101 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = events_df.drop(['event','transactionid'],axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不重复的用户数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1407580"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_df.visitorid.nunique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将目标放在活动交互次数大于10次的用户"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users = events_df['visitorid']\n",
    "# users = users.value_counts()\n",
    "# users = users.iloc[users.values > 10].index\n",
    "# users = users.sort_values()\n",
    "# users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./data/retailrocket/SLdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = dataset.groupby('visitorid',group_keys=False).groups"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "做出每个用户访问物品的列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list=[]\n",
    "action_list=[]\n",
    "for user in dataset.visitorid.unique().tolist():\n",
    "    item_list=[]\n",
    "    for item in group[user]:\n",
    "        item = dataset['itemid'][item]\n",
    "        item_list.append(item)\n",
    "    user_list.append(user)\n",
    "    action_list.append(item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'user':user_list, 'actions':action_list}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('./data/retailrocket/seqData.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_iter_random(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"使用随机抽样生成一个小批量子序列\"\"\"\n",
    "    # 减去1，是因为我们需要考虑标签\n",
    "    num_subseqs = (len(corpus) - 1) // num_steps\n",
    "    # 长度为num_steps的子序列的起始索引\n",
    "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
    "    # 在随机抽样的迭代过程中，\n",
    "    # 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻\n",
    "    random.shuffle(initial_indices)\n",
    "\n",
    "    def data(pos):\n",
    "        # 返回从pos位置开始的长度为num_steps的序列\n",
    "        return corpus[pos: pos + num_steps]\n",
    "\n",
    "    num_batches = num_subseqs // batch_size\n",
    "    for i in range(0, batch_size * num_batches, batch_size):\n",
    "        # 在这里，initial_indices包含子序列的随机起始索引\n",
    "        initial_indices_per_batch = initial_indices[i: i + batch_size]\n",
    "        X = [data(j) for j in initial_indices_per_batch]\n",
    "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
    "        yield tf.constant(X), tf.constant(Y)\n",
    "        #yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_loader(df, batch_size, num_steps):\n",
    "    for index in df.index:\n",
    "        seq_data = df.loc[index].actions\n",
    "        x_list=[]\n",
    "        y_list=[]\n",
    "        for X,Y in seq_data_iter_random(seq_data, batch_size, num_steps):\n",
    "            x_list.append(X)\n",
    "            y_list.append(Y)\n",
    "        yield index, [x_list, y_list]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_loader(df, batch_size, num_steps):\n",
    "    for index in df.index:\n",
    "        seq_data = df.loc[index].actions\n",
    "        action_and_label = seq_data_iter_random(seq_data, batch_size, num_steps)\n",
    "        yield index, action_and_label      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'item_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m embed \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mEmbedding(\n\u001b[0;32m----> 2\u001b[0m     input_dim\u001b[39m=\u001b[39mitem_size, \n\u001b[1;32m      3\u001b[0m     output_dim\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[39m#embeddings_initializer=tf.keras.initializers.GlorotUniform(),\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     input_length\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'item_size' is not defined"
     ]
    }
   ],
   "source": [
    "embed = tf.keras.layers.Embedding(\n",
    "    input_dim=item_size, \n",
    "    output_dim=64,\n",
    "    #embeddings_initializer=tf.keras.initializers.GlorotUniform(),\n",
    "    input_length=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_embedded = embed(item_properties_df.itemid.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = tf.nn.embedding_lookup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataHelper import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Data(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_size = dataset.item_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import tensorflow as d2l\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from numpy import random\n",
    "from CFN_impl import CFNCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(tf.keras.Model):\n",
    "    def __init__(self, rnn_layer, item_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.dense = tf.keras.layers.Dense(item_size)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "        self.input_embed = tf.keras.layers.Embedding(\n",
    "            input_dim=item_size, output_dim=64\n",
    "        )\n",
    "        # self.output_embed = tf.keras.layers.Embedding(\n",
    "        #     input_dim=256, output_dim=item_size\n",
    "        # )\n",
    "        # self.state_embed = tf.keras.layers.Embedding(\n",
    "        #     input_dim=1, output_dim=32\n",
    "        # )\n",
    "\n",
    "\n",
    "    def call(self, inputs, state):\n",
    "        X = self.input_embed(inputs)\n",
    "        #X = inputs\n",
    "        # rnn返回两个以上的值\n",
    "        Y, *state = self.rnn(X, state)\n",
    "        output_ = self.dropout(Y)\n",
    "        output = self.dense(output_)\n",
    "        #output = self.output_embed(output)\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.cell.get_initial_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 11:22:37.629289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 11:22:37.668062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 11:22:37.668148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 11:22:37.668624: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-01 11:22:37.669180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 11:22:37.669244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 11:22:37.669286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 11:22:38.029593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 11:22:38.029675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 11:22:38.029723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 11:22:38.029920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5742 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "cfn_cell = CFNCell(256)\n",
    "cfn_layer = tf.keras.layers.RNN(cfn_cell,\n",
    "                                return_state=True,\n",
    "                                return_sequences=True)\n",
    "net = RNNModel(cfn_layer, item_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(grads, theta):  #@save\n",
    "    \"\"\"裁剪梯度\"\"\"\n",
    "    theta = tf.constant(theta, dtype=tf.float32)\n",
    "    new_grad = []\n",
    "    for grad in grads:\n",
    "        if isinstance(grad, tf.IndexedSlices):\n",
    "            new_grad.append(tf.convert_to_tensor(grad))\n",
    "        else:\n",
    "            new_grad.append(grad)\n",
    "    norm = tf.math.sqrt(sum((tf.reduce_sum(grad ** 2)).numpy()\n",
    "                        for grad in new_grad))\n",
    "    norm = tf.cast(norm, tf.float32)\n",
    "    if tf.greater(norm, theta):\n",
    "        for i, grad in enumerate(new_grad):\n",
    "            new_grad[i] = grad * theta / norm\n",
    "    else:\n",
    "        new_grad = new_grad\n",
    "    return new_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def train_epoch_ch8(net, train_iter, loss, updater, item_size, use_random_iter):\n",
    "    \"\"\"训练模型一个迭代周期(定义见第8章)\"\"\"\n",
    "    state = None\n",
    "    metric = d2l.Accumulator(2)  # 训练损失之和,词元数量\n",
    "    #print(next(train_iter))\n",
    "    for X, Y in train_iter:\n",
    "        #print('AAAAAAAAAAA!')\n",
    "        #X = lookup(item_embedded, X)\n",
    "        #Y = lookup(item_embedded, Y)\n",
    "        if state is None or use_random_iter:\n",
    "            # 在第一次迭代或使用随机抽样时初始化state\n",
    "            state = net.begin_state(batch_size=X.shape[0], dtype=tf.float32)\n",
    "        with tf.GradientTape(persistent=True) as g:\n",
    "            y_hat, state = net(X, state)\n",
    "            y = tf.one_hot(Y, item_size)\n",
    "            l = loss(y, y_hat)\n",
    "        params = net.trainable_variables\n",
    "        grads = g.gradient(l, params)\n",
    "        #grads = grad_clipping(grads, 1)\n",
    "        updater.apply_gradients(zip(grads, params))\n",
    "        # Keras默认返回一个批量中的平均损失\n",
    "        metric.add(l , 1)\n",
    "        #print('immediate loss:',l)\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def train_ch8(net, dataset, lr, num_epochs, strategy,\n",
    "              use_random_iter=False):\n",
    "    \"\"\"训练模型(定义见第8章)\"\"\"\n",
    "    with strategy.scope():\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        updater = tf.keras.optimizers.SGD(lr)\n",
    "    #animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',\n",
    "     #                      legend=['train'], xlim=[10, num_epochs])\n",
    "    #predict = lambda prefix: predict_ch8(prefix, 50, net, vocab)\n",
    "    # 训练和预测\n",
    "    for epoch in range(num_epochs):\n",
    "        df = dataset.df\n",
    "        train_iter = dataset.dataset_loader(df, 1, 3)\n",
    "        item_size = dataset.item_size\n",
    "        metric = d2l.Accumulator(2) \n",
    "        for user, actions in train_iter:\n",
    "            avg_loss = train_epoch_ch8(net, actions, loss, updater, item_size,\n",
    "                                        use_random_iter)\n",
    "            #print('userid:',user, 'avg_loss:',avg_loss)\n",
    "            metric.add(avg_loss , 1)\n",
    "            if metric[1] % 1000 == 0:\n",
    "                print('epoch:',epoch, 'metric0:',metric[0],'\\nmetric1:',metric[1], 'loss:', metric[0]/metric[1])\n",
    "        # if (epoch + 1) % 10 == 0:\n",
    "        #     #print(predict('time traveller'))\n",
    "        #animator.add(epoch, [metric[0] / metric[1]])\n",
    "        #device = d2l.try_gpu()._device_name\n",
    "        #print(f'avg_loss {avg_loss:.1f}, {speed:.1f} 词元/秒 {str(device)}')\n",
    "    #print(predict('time traveller'))\n",
    "    #print(predict('traveller'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = d2l.try_gpu()._device_name\n",
    "strategy = tf.distribute.OneDeviceStrategy(device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 metric0: 80.66035545887623 \n",
      "metric1: 1000.0 loss: 0.08066035545887623\n",
      "epoch: 0 metric0: 161.21433961096824 \n",
      "metric1: 2000.0 loss: 0.08060716980548412\n",
      "epoch: 0 metric0: 241.65668787058254 \n",
      "metric1: 3000.0 loss: 0.08055222929019418\n",
      "epoch: 0 metric0: 321.9673492880946 \n",
      "metric1: 4000.0 loss: 0.08049183732202365\n",
      "epoch: 0 metric0: 402.12291196082526 \n",
      "metric1: 5000.0 loss: 0.08042458239216506\n",
      "epoch: 0 metric0: 482.1777900880955 \n",
      "metric1: 6000.0 loss: 0.08036296501468258\n",
      "epoch: 0 metric0: 562.1176471600505 \n",
      "metric1: 7000.0 loss: 0.08030252102286436\n",
      "epoch: 0 metric0: 641.9064472170838 \n",
      "metric1: 8000.0 loss: 0.08023830590213547\n",
      "epoch: 0 metric0: 721.5702013660066 \n",
      "metric1: 9000.0 loss: 0.08017446681844517\n",
      "epoch: 0 metric0: 801.1212357881288 \n",
      "metric1: 10000.0 loss: 0.08011212357881288\n",
      "epoch: 0 metric0: 880.5495746119254 \n",
      "metric1: 11000.0 loss: 0.08004996132835686\n",
      "epoch: 0 metric0: 959.8463301502162 \n",
      "metric1: 12000.0 loss: 0.07998719417918468\n",
      "epoch: 0 metric0: 1039.0362181892997 \n",
      "metric1: 13000.0 loss: 0.07992586293763844\n",
      "epoch: 0 metric0: 1118.1114910722176 \n",
      "metric1: 14000.0 loss: 0.0798651065051584\n",
      "epoch: 0 metric0: 1197.0513085305124 \n",
      "metric1: 15000.0 loss: 0.07980342056870082\n",
      "epoch: 0 metric0: 1275.8806056667884 \n",
      "metric1: 16000.0 loss: 0.07974253785417428\n",
      "epoch: 0 metric0: 1354.5733116827907 \n",
      "metric1: 17000.0 loss: 0.07968078304016415\n",
      "epoch: 0 metric0: 1433.133410202744 \n",
      "metric1: 18000.0 loss: 0.07961852278904133\n",
      "epoch: 0 metric0: 1511.5771759808447 \n",
      "metric1: 19000.0 loss: 0.07955669347267603\n",
      "epoch: 0 metric0: 1589.9142540856317 \n",
      "metric1: 20000.0 loss: 0.07949571270428159\n",
      "epoch: 0 metric0: 1668.15595500419 \n",
      "metric1: 21000.0 loss: 0.07943599785734239\n",
      "epoch: 0 metric0: 1746.290551630812 \n",
      "metric1: 22000.0 loss: 0.079376843255946\n",
      "epoch: 0 metric0: 1824.3131803596254 \n",
      "metric1: 23000.0 loss: 0.07931796436346197\n",
      "epoch: 0 metric0: 1902.2195379577354 \n",
      "metric1: 24000.0 loss: 0.07925914741490564\n",
      "epoch: 0 metric0: 1979.9565357328388 \n",
      "metric1: 25000.0 loss: 0.07919826142931355\n",
      "epoch: 0 metric0: 2057.576023878865 \n",
      "metric1: 26000.0 loss: 0.07913753937995634\n",
      "epoch: 0 metric0: 2135.088428746696 \n",
      "metric1: 27000.0 loss: 0.0790773492128406\n",
      "epoch: 0 metric0: 2212.497610452499 \n",
      "metric1: 28000.0 loss: 0.07901777180187497\n",
      "epoch: 0 metric0: 2289.803613539391 \n",
      "metric1: 29000.0 loss: 0.07895874529446176\n",
      "epoch: 0 metric0: 2366.982878427475 \n",
      "metric1: 30000.0 loss: 0.07889942928091584\n",
      "epoch: 0 metric0: 2444.0610665841864 \n",
      "metric1: 31000.0 loss: 0.07884067956723181\n",
      "epoch: 0 metric0: 2521.0128611463065 \n",
      "metric1: 32000.0 loss: 0.07878165191082208\n",
      "epoch: 0 metric0: 2597.850628281467 \n",
      "metric1: 33000.0 loss: 0.07872274631155961\n",
      "epoch: 0 metric0: 2674.579941535095 \n",
      "metric1: 34000.0 loss: 0.07866411592750279\n",
      "epoch: 0 metric0: 2751.190886359537 \n",
      "metric1: 35000.0 loss: 0.07860545389598678\n",
      "epoch: 0 metric0: 2827.6935700282556 \n",
      "metric1: 36000.0 loss: 0.07854704361189599\n",
      "epoch: 0 metric0: 2904.0889945443764 \n",
      "metric1: 37000.0 loss: 0.07848889174444261\n",
      "epoch: 0 metric0: 2980.37161578362 \n",
      "metric1: 38000.0 loss: 0.0784308319943058\n",
      "epoch: 0 metric0: 3056.5354908063946 \n",
      "metric1: 39000.0 loss: 0.07837270489247165\n",
      "epoch: 0 metric0: 3132.5780156909277 \n",
      "metric1: 40000.0 loss: 0.0783144503922732\n",
      "epoch: 0 metric0: 3208.4760726212094 \n",
      "metric1: 41000.0 loss: 0.07825551396637095\n",
      "epoch: 0 metric0: 3284.233452635828 \n",
      "metric1: 42000.0 loss: 0.07819603458656733\n",
      "epoch: 0 metric0: 3359.886393174356 \n",
      "metric1: 43000.0 loss: 0.0781368928645199\n",
      "epoch: 0 metric0: 3435.4247104009232 \n",
      "metric1: 44000.0 loss: 0.0780778343272937\n",
      "epoch: 0 metric0: 3510.8428780599543 \n",
      "metric1: 45000.0 loss: 0.07801873062355454\n",
      "epoch: 0 metric0: 3586.1497974552667 \n",
      "metric1: 46000.0 loss: 0.07795977820554928\n",
      "epoch: 0 metric0: 3661.3460836610416 \n",
      "metric1: 47000.0 loss: 0.07790098050342642\n",
      "epoch: 0 metric0: 3736.447755062122 \n",
      "metric1: 48000.0 loss: 0.0778426615637942\n",
      "epoch: 0 metric0: 3811.456875649005 \n",
      "metric1: 49000.0 loss: 0.07778483419691846\n",
      "epoch: 0 metric0: 3886.3649924066 \n",
      "metric1: 50000.0 loss: 0.077727299848132\n",
      "epoch: 0 metric0: 3961.1569899698084 \n",
      "metric1: 51000.0 loss: 0.0776697449013688\n",
      "epoch: 0 metric0: 4035.8242441134175 \n",
      "metric1: 52000.0 loss: 0.0776120046944888\n",
      "epoch: 0 metric0: 4110.31784139636 \n",
      "metric1: 53000.0 loss: 0.07755316681879924\n",
      "epoch: 0 metric0: 4184.706171150726 \n",
      "metric1: 54000.0 loss: 0.07749455872501344\n",
      "epoch: 0 metric0: 4258.992306174345 \n",
      "metric1: 55000.0 loss: 0.07743622374862445\n",
      "epoch: 0 metric0: 4333.178319376403 \n",
      "metric1: 56000.0 loss: 0.07737818427457863\n",
      "epoch: 0 metric0: 4407.268210277946 \n",
      "metric1: 57000.0 loss: 0.07732049491715695\n",
      "epoch: 0 metric0: 4481.266754501031 \n",
      "metric1: 58000.0 loss: 0.07726321990519018\n",
      "epoch: 0 metric0: 4555.159579910237 \n",
      "metric1: 59000.0 loss: 0.07720609457474978\n",
      "epoch: 0 metric0: 4628.941619423298 \n",
      "metric1: 60000.0 loss: 0.07714902699038831\n",
      "epoch: 0 metric0: 4702.612269874798 \n",
      "metric1: 61000.0 loss: 0.077092004424177\n",
      "epoch: 0 metric0: 4776.179658878381 \n",
      "metric1: 62000.0 loss: 0.07703515578836098\n",
      "epoch: 0 metric0: 4849.645085381292 \n",
      "metric1: 63000.0 loss: 0.07697849341875067\n",
      "epoch: 1 metric0: 73.33720863553995 \n",
      "metric1: 1000.0 loss: 0.07333720863553995\n",
      "epoch: 1 metric0: 146.57735526314957 \n",
      "metric1: 2000.0 loss: 0.07328867763157479\n",
      "epoch: 1 metric0: 219.71566164547446 \n",
      "metric1: 3000.0 loss: 0.07323855388182482\n",
      "epoch: 1 metric0: 292.7338180516282 \n",
      "metric1: 4000.0 loss: 0.07318345451290705\n",
      "epoch: 1 metric0: 365.61044829414334 \n",
      "metric1: 5000.0 loss: 0.07312208965882867\n",
      "epoch: 1 metric0: 438.39523133555963 \n",
      "metric1: 6000.0 loss: 0.07306587188925993\n",
      "epoch: 1 metric0: 511.0750565075304 \n",
      "metric1: 7000.0 loss: 0.07301072235821864\n",
      "epoch: 1 metric0: 583.6170566828098 \n",
      "metric1: 8000.0 loss: 0.07295213208535123\n",
      "epoch: 1 metric0: 656.0449245344169 \n",
      "metric1: 9000.0 loss: 0.07289388050382409\n",
      "epoch: 1 metric0: 728.3699678764783 \n",
      "metric1: 10000.0 loss: 0.07283699678764784\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_ch8(net, dataset, \u001b[39m1\u001b[39;49m, \u001b[39m10\u001b[39;49m, strategy, \u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[33], line 18\u001b[0m, in \u001b[0;36mtrain_ch8\u001b[0;34m(net, dataset, lr, num_epochs, strategy, use_random_iter)\u001b[0m\n\u001b[1;32m     16\u001b[0m metric \u001b[39m=\u001b[39m d2l\u001b[39m.\u001b[39mAccumulator(\u001b[39m2\u001b[39m) \n\u001b[1;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m user, actions \u001b[39min\u001b[39;00m train_iter:\n\u001b[0;32m---> 18\u001b[0m     avg_loss \u001b[39m=\u001b[39m train_epoch_ch8(net, actions, loss, updater, item_size,\n\u001b[1;32m     19\u001b[0m                                 use_random_iter)\n\u001b[1;32m     20\u001b[0m     \u001b[39m#print('userid:',user, 'avg_loss:',avg_loss)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     metric\u001b[39m.\u001b[39madd(avg_loss , \u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 19\u001b[0m, in \u001b[0;36mtrain_epoch_ch8\u001b[0;34m(net, train_iter, loss, updater, item_size, use_random_iter)\u001b[0m\n\u001b[1;32m     17\u001b[0m     l \u001b[39m=\u001b[39m loss(y, y_hat)\n\u001b[1;32m     18\u001b[0m params \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39mtrainable_variables\n\u001b[0;32m---> 19\u001b[0m grads \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39;49mgradient(l, params)\n\u001b[1;32m     20\u001b[0m \u001b[39m#grads = grad_clipping(grads, 1)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m updater\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(grads, params))\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py:1081\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1077\u001b[0m \u001b[39mif\u001b[39;00m output_gradients \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1078\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1079\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m nest\u001b[39m.\u001b[39mflatten(output_gradients)]\n\u001b[0;32m-> 1081\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[1;32m   1082\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[1;32m   1083\u001b[0m     flat_targets,\n\u001b[1;32m   1084\u001b[0m     flat_sources,\n\u001b[1;32m   1085\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[1;32m   1086\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[1;32m   1087\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[1;32m   1089\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[1;32m   1090\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[1;32m     68\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m     target,\n\u001b[1;32m     70\u001b[0m     sources,\n\u001b[1;32m     71\u001b[0m     output_gradients,\n\u001b[1;32m     72\u001b[0m     sources_raw,\n\u001b[1;32m     73\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py:156\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    154\u001b[0m     gradient_name_scope \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m forward_pass_name_scope \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[0;32m--> 156\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39;49mout_grads)\n\u001b[1;32m    157\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39mout_grads)\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/tensorflow/python/ops/math_grad.py:1741\u001b[0m, in \u001b[0;36m_MatMulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1739\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m t_a \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m t_b:\n\u001b[1;32m   1740\u001b[0m   grad_a \u001b[39m=\u001b[39m gen_math_ops\u001b[39m.\u001b[39mmat_mul(grad, b, transpose_b\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m-> 1741\u001b[0m   grad_b \u001b[39m=\u001b[39m gen_math_ops\u001b[39m.\u001b[39;49mmat_mul(a, grad, transpose_a\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m t_a \u001b[39mand\u001b[39;00m t_b:\n\u001b[1;32m   1743\u001b[0m   grad_a \u001b[39m=\u001b[39m gen_math_ops\u001b[39m.\u001b[39mmat_mul(grad, b)\n",
      "File \u001b[0;32m~/anaconda3/envs/d2l/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py:6013\u001b[0m, in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   6011\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   6012\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 6013\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   6014\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMatMul\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, a, b, \u001b[39m\"\u001b[39;49m\u001b[39mtranspose_a\u001b[39;49m\u001b[39m\"\u001b[39;49m, transpose_a, \u001b[39m\"\u001b[39;49m\u001b[39mtranspose_b\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   6015\u001b[0m       transpose_b)\n\u001b[1;32m   6016\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   6017\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_ch8(net, dataset, 1, 10, strategy, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
